{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "MuyjkQ3nudCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5pYGWAvuLo-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.30.2\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate\n",
        "!pip install datasets\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version # 3.10.12"
      ],
      "metadata": {
        "id": "yTxqRpzYuZgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive Preparation"
      ],
      "metadata": {
        "id": "o4Q1iPApUyjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t4a_2OPDUwu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a63fc49-168a-42ad-a7ac-6d048cb77218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "2fJKI2wyunuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "from scipy import stats\n",
        "import torch\n",
        "import tqdm.notebook\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, logging, LlamaTokenizer, LlamaForCausalLM\n",
        "from transformers.models.cvt.modeling_cvt import CrossEntropyLoss\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib.ticker as ptick\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
      ],
      "metadata": {
        "id": "C_Mr4tQQub_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__ # 2.1.0+cu121"
      ],
      "metadata": {
        "id": "HtO9g7PMvVEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huggingface Login"
      ],
      "metadata": {
        "id": "F4JeypETvhFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "PMR5-fccvjr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "vdf25yoFvo8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'openlm-research/open_llama_7b_v2'\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
        "model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map='auto')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "rZqW36wHvl7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Raw Preambles"
      ],
      "metadata": {
        "id": "9zupeUJH06e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pooled preamble subsets (unsorted)\n",
        "BASE_CC_PATH = '' # raw_preamble_randomly_generated.pkl\n",
        "with open (BASE_CC_PATH, 'rb') as f:\n",
        "    base_cc = pickle.load(f)\n",
        "\n",
        "# N- preambles for each type (rondmuly selected and orderted 10 preambles per type)\n",
        "RAND_CC_PATH = '' # raw_preamble_random_ordered.pkl\n",
        "with open (RAND_CC_PATH, 'rb') as f:\n",
        "    rand_cc = pickle.load(f)"
      ],
      "metadata": {
        "id": "a-ZaURmf04_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Evalation Data"
      ],
      "metadata": {
        "id": "UIq0U0Xlwhn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "data_cp = load_dataset(\"crows_pairs\")\n",
        "records = []\n",
        "num_example = 0\n",
        "\n",
        "for example in data_cp['test']:\n",
        "    if example['bias_type'] == 2:\n",
        "        record = {}\n",
        "        record['stereotype'] = example['sent_more']\n",
        "        record['anti-stereotype'] = example['sent_less']\n",
        "        record['stereo_antistereo'] = example['stereo_antistereo']\n",
        "        records.append(record)"
      ],
      "metadata": {
        "id": "twO136TnvupY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sort and Select N-Preambles based on Pre-computed Perplexity"
      ],
      "metadata": {
        "id": "pAyZFBAgyKi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing perplexity for a given input text\n",
        "def calc_perplexity(model, tokenizer, input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    label_ids = input_ids.clone()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, labels=label_ids)\n",
        "        shift_logits = outputs.logits[:,:-1,:].contiguous()\n",
        "        shift_labels = label_ids[:,1:].contiguous()\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        if shift_logits.dtype != torch.float32:\n",
        "            shift_logits = shift_logits.to(torch.float32)\n",
        "            assert shift_logits.dtype == torch.float32\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        perplexity = torch.exp(loss)\n",
        "        return perplexity.item()\n",
        "\n",
        "# for sorting preambles based of perplexity\n",
        "def sort_select_preambles(model, tokenizer, save_name, base_cc):\n",
        "    seed = 0\n",
        "    torch.cuda.empty_cache()\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    perp_DB = {} # for storing perplexity values of preambles\n",
        "    model.to('cuda')\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "    for key in base_cc.keys(): # NOTE: key represents preamble type (e.g., CF-simple)\n",
        "        print('------  preamble type  ------- : ', key)\n",
        "        perp_DB[key] = {}\n",
        "        for input_sentence in tqdm.notebook.tqdm(base_cc[key]):\n",
        "            with torch.no_grad():\n",
        "                input_perplexity  = calc_perplexity(model, tokenizer, input_sentence)\n",
        "                perp_DB[key][input_sentence] = input_perplexity\n",
        "\n",
        "    SAVE_PATH = save_name + '_perp_DB.pkl'\n",
        "    print('saving perplexity values of preambles into ... ', SAVE_PATH)\n",
        "    with open (SAVE_PATH, 'wb') as f:\n",
        "        pickle.dump(perp_DB, f)\n",
        "\n",
        "    sorted_cc = {} # for storing sorted preambles based on the computed perplexity values\n",
        "    for k in perp_DB.keys():\n",
        "        tmp = sorted(perp_DB[k].items(), key=lambda x:x[1])[:15] # 15 preambles per type at maximum\n",
        "        sorted_cc[k] = [x for x, y in tmp]\n",
        "\n",
        "    SAVE_PATH = save_name + '_sorted_cc.pkl'\n",
        "    print('saving low-perplexity premables into ... ', SAVE_PATH)\n",
        "    with open (SAVE_PATH, 'wb') as f:\n",
        "        pickle.dump(sorted_cc, f)\n",
        "\n",
        "    return perp_DB, sorted_cc\n",
        "\n"
      ],
      "metadata": {
        "id": "QtZSzeH_yJSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "SAVE_PATH_PREFIX = ''\n",
        "_, _ = sort_select_preambles(model, tokenizer, SAVE_PATH_PREFIX, base_cc)"
      ],
      "metadata": {
        "id": "E2E467h_LTU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downstream Evaluation"
      ],
      "metadata": {
        "id": "gfe3cfxR-Mws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### You need to download them preliminary ####\n",
        "# https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/language_understanding/hellaswag.jsonl\n",
        "# https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/commonsense_reasoning/copa.jsonl\n",
        "COPA_PATH = ''\n",
        "HS_PATH = ''\n",
        "########################################\n",
        "\n",
        "res_copa = []\n",
        "decoder = json.JSONDecoder()\n",
        "with open (COPA_PATH, 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        res_copa.append(data)\n",
        "\n",
        "res_hella = []\n",
        "decoder = json.JSONDecoder()\n",
        "with open (HS_PATH, 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        res_hella.append(data)\n",
        "\n",
        "def _tokenizer_needs_prefix_space(tokenizer) -> bool:\n",
        "    return len(tokenizer(' a', add_special_tokens=False)['input_ids']) == 1\n",
        "\n",
        "def _make_padded_input(context_enc, continuation_enc, max_seq_len, pad_tok_id):\n",
        "    if len(continuation_enc) + len(context_enc) > max_seq_len:\n",
        "        context_max_subseq_len = max_seq_len - len(continuation_enc)\n",
        "        if context_max_subseq_len < 0:\n",
        "            raise Exception(f'Dataset included continuation longer than the max seq len')\n",
        "        context_enc = context_enc[-(context_max_subseq_len):]\n",
        "    continuation_span = torch.tensor(range(len(context_enc), len(context_enc) + len(continuation_enc)))\n",
        "    inp = torch.tensor((context_enc + continuation_enc), dtype=torch.long,)\n",
        "    return inp, continuation_span\n",
        "\n",
        "def run_glue(db_key, task='copa'):\n",
        "    all_res_sum = []\n",
        "    for num_index in tqdm.notebook.tqdm([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]):\n",
        "\n",
        "        if db_key == \"NONE\":\n",
        "            cf_context = None\n",
        "        else: # db_key in ['t1', 't2', 't1_rand', 't2_rand']:\n",
        "            LOAD_PATH = SAVE_PATH_PREFIX + '_sorted_cc.pkl'\n",
        "            with open (LOAD_PATH, 'rb') as f:\n",
        "                cc_sort_DB = pickle.load(f)\n",
        "            cf_context = cc_sort_DB[db_key][:num_index]\n",
        "\n",
        "        if task == 'hella':\n",
        "            examples = []\n",
        "            for sample in res_hella:\n",
        "                choices = [(f'{choice}' if not choice.startswith(' ') else choice) for choice in sample['choices']]\n",
        "                encoded_example = {}\n",
        "                preamble = ' '\n",
        "                encoded_example['preamble'] = tokenizer(preamble, add_special_tokens=False)\n",
        "                encoded_example['gold_idx'] = sample['gold']\n",
        "                encoded_example['query'] = tokenizer(sample['query'], add_special_tokens=False)\n",
        "                encoded_example['choices'] = [tokenizer(choice, add_special_tokens=False) for choice in choices]\n",
        "                examples.append(encoded_example)\n",
        "        else:\n",
        "            examples = []\n",
        "            for sample in res_copa:\n",
        "                choices = [(f'{choice}' if not choice.startswith(' ') else choice) for choice in sample['choices']]\n",
        "                encoded_example = {}\n",
        "                preamble = ' '\n",
        "                encoded_example['preamble'] = tokenizer(preamble, add_special_tokens=False)\n",
        "                encoded_example['gold_idx'] = sample['gold']\n",
        "                encoded_example['query'] = tokenizer(sample['query'], add_special_tokens=False)\n",
        "                encoded_example['choices'] = [tokenizer(choice, add_special_tokens=False) for choice in choices]\n",
        "                examples.append(encoded_example)\n",
        "\n",
        "        num_correct = 0\n",
        "        num_total = 0\n",
        "        seq_rec = []\n",
        "\n",
        "        for example in examples[:100]:\n",
        "            preamble, context, choices, gold_idx = (example['preamble'], example['query'], example['choices'], example['gold_idx'])\n",
        "            inputs = []\n",
        "            choice_spans = []\n",
        "            res_perplexity = []\n",
        "            for choice in choices:\n",
        "                if cf_context:\n",
        "                    cf_comb = ' '.join(cf_context)\n",
        "                    cf_ids = tokenizer(cf_comb, add_special_tokens=False)\n",
        "                    if len(preamble['input_ids']) == 0:\n",
        "                        context_enc = cf_ids['input_ids'] + context['input_ids']\n",
        "                    else:\n",
        "                        context_enc = cf_ids['input_ids'] + preamble['input_ids'] + context['input_ids']\n",
        "                else:\n",
        "                    context_enc = context['input_ids']\n",
        "\n",
        "                continuation_enc = choice['input_ids']\n",
        "\n",
        "                input_ids, choice_span = _make_padded_input(context_enc, continuation_enc, tokenizer.model_max_length, tokenizer.pad_token_id)\n",
        "                input_ids.unsqueeze_(0)\n",
        "                label_ids = input_ids.clone()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_ids=input_ids, labels=label_ids)\n",
        "                    shift_logits = outputs.logits[:,-1*len(choice_span):-1,:].contiguous()\n",
        "                    shift_labels = label_ids[:,-1*len(choice_span)+1:].contiguous()\n",
        "\n",
        "                    loss_fct = CrossEntropyLoss()\n",
        "\n",
        "                    if shift_logits.dtype != torch.float32:\n",
        "                        shift_logits = shift_logits.to(torch.float32)\n",
        "                        assert shift_logits.dtype == torch.float32\n",
        "\n",
        "                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "                    perplexity = torch.exp(loss)\n",
        "                    perplexity = perplexity.item()\n",
        "                    res_perplexity.append(perplexity)\n",
        "\n",
        "            idx_min = res_perplexity.index(min(res_perplexity))\n",
        "            if idx_min == gold_idx:\n",
        "                num_correct += 1\n",
        "            num_total += 1\n",
        "        all_res_sum.append(num_correct)\n",
        "        if db_key == \"NONE\":\n",
        "            break\n",
        "    return all_res_sum"
      ],
      "metadata": {
        "id": "nRXxW6gxA1JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copa_srt_res = {}\n",
        "hella_srt_res = {}\n",
        "\n",
        "result_hella = run_glue('NONE', 'hella')[0]\n",
        "result_copa = run_glue('NONE', 'copa')[0]\n",
        "hella_srt_res['NONE'] = [result_hella for _ in range(10)]\n",
        "copa_srt_res['NONE'] = [result_copa for _ in range(10)]\n",
        "for k in ['t1', 't2', 't1_rand', 't2_rand']:\n",
        "    copa_srt_res[k] = run_glue(k, 'copa')\n",
        "for k in ['t1', 't2', 't1_rand', 't2_rand']:\n",
        "    hella_srt_res[k] = run_glue(k, 'hella')"
      ],
      "metadata": {
        "id": "t68D5zebFyvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "fig = plt.figure(figsize=(6,2.5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.set_xlabel(\"number of preambles\", fontsize=15)\n",
        "ax.set_ylabel(\"accuracy (hellaswag)\", fontsize=18)\n",
        "\n",
        "ax.set_xticks(num_index)\n",
        "ax.set_yticks([50, 55, 60, 65, 70])\n",
        "ax.tick_params(labelsize=12)\n",
        "\n",
        "ax.plot(num_index, hella_srt_res['NONE'], label='nc', color='black')\n",
        "ax.plot(num_index, hella_srt_res['t1'], label='CF-simple', linestyle = \"dotted\", linewidth = 1, color='c')\n",
        "ax.plot(num_index, hella_srt_res['t2'], label='CF-detailed', linestyle = \"dashed\", linewidth = 1, color = 'b')\n",
        "ax.plot(num_index, hella_srt_res['t1_rand'], label='Desc-simple', linestyle =  (0, (5, 5)), linewidth = 1, color = 'lightcoral')\n",
        "ax.plot(num_index, hella_srt_res['t2_rand'], label='Desc-detailed', linestyle = (0, (5, 10)), linewidth =1, color='red')\n",
        "\n",
        "ax.legend(fontsize=10, loc='lower right', ncol=3)\n",
        "ax.set_xlim(1, 10)\n",
        "ax.set_ylim(50, 70)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xmzvNZrrG41V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "fig = plt.figure(figsize=(6,2.5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.set_xlabel(\"number of preambles\", fontsize=15)\n",
        "ax.set_ylabel(\"accuracy (copa)\", fontsize=18)\n",
        "\n",
        "ax.set_xticks(num_index)\n",
        "ax.set_yticks([70, 75, 80, 85, 90])\n",
        "ax.tick_params(labelsize=12)\n",
        "\n",
        "ax.plot(num_index, copa_srt_res['NONE'], label='nc', color='black')\n",
        "ax.plot(num_index, copa_srt_res['t1'], label='CF-simple', linestyle = \"dotted\", linewidth = 1, color='c')\n",
        "ax.plot(num_index, copa_srt_res['t2'], label='CF-detailed', linestyle = \"dashed\", linewidth = 1, color = 'b')\n",
        "ax.plot(num_index, copa_srt_res['t1_rand'], label='Desc-simple', linestyle =  (0, (5, 5)), linewidth = 1, color = 'lightcoral')\n",
        "ax.plot(num_index, copa_srt_res['t2_rand'], label='Desc-detailed', linestyle = (0, (5, 10)), linewidth =1, color='red')\n",
        "\n",
        "ax.legend(fontsize=10, loc='upper right', ncol=3)\n",
        "ax.set_xlim(1, 10)\n",
        "ax.set_ylim(70, 90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gFk9aKX_HLmG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}